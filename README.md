# deepseek-r1:8b and llama4:scout running together
Dockerized Open-WebUI Ollama setup to run two (and more) models simultaneously
Entrypoint script downloads deepseek-r1:8b and llama4:scout models automatically

## ðŸ“‹ Prerequisites
- Docker Engine v20.10.10+
- Docker Compose v2.20.0+
- 32GB RAM minimum (>80GB recommended for faster replies)
- 30GB+ free disk space

## ðŸš€ Quick Start
1. **Clone the repo**
```bash
git clone https://github.com/svirmi/ollama-open-webui.git
```